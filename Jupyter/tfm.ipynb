{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS ###\n",
    "import scipy.sparse\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pyplot\n",
    "import sklearn.metrics as mtc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from pyemd import emd\n",
    "import valores\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import fashion_mnist  # new with Keras 2.1.2.  Yah!!\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras import backend as K\n",
    "import sparse\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCIONES ##\n",
    "\n",
    "\n",
    "class TFM:\n",
    "\n",
    "    def __init__(self,path):\n",
    "        self.X_tr = scipy.sparse.load_npz(path+'X_tr.npz')\n",
    "        self.X_tst = scipy.sparse.load_npz(path+'X_tst.npz')\n",
    "        self.y_tr = np.load(path+'y_tr.npy')\n",
    "        self.y_tst = np.load(path+'y_tst.npy')\n",
    "        # Se binarizan las caracteristicas \n",
    "        self.y_tr=MultiLabelBinarizer().fit_transform(self.y_tr)\n",
    "        self.y_tst=MultiLabelBinarizer().fit_transform(self.y_tst)\n",
    "        # Escalado \n",
    "        scaler = MaxAbsScaler().fit(self.X_tr)\n",
    "        self.X_tr=scaler.transform(self.X_tr)\n",
    "        self.X_tst=scaler.transform(self.X_tst)\n",
    "        self.X_tr_simpli= None\n",
    "        self.simplificador = False\n",
    "        self.modelo = None\n",
    "        self.clf= None\n",
    "        self.columns= None\n",
    "        print ('Dataset cargado y preprocesado')\n",
    "\n",
    "    def reduce_features(self,num_data):\n",
    "        columns=np.array([])\n",
    "        for i in range(np.shape(self.X_tr)[1]):\n",
    "            if self.X_tr[:,i].count_nonzero()> num_data:\n",
    "                columns=np.append(columns,i)\n",
    "        np.savetxt('features'+num_data+'.txt',columns,delimiter=',')\n",
    "        print('Las características han sido reducidas')\n",
    "\n",
    "    def simplify_dataset(self,features,tr_size):\n",
    "        self.X_tr_simpli=self.X_tr\n",
    "        self.y_tr_simpli=self.y_tr\n",
    "        \n",
    "        self.columns=np.loadtxt('features'+features+'.txt',delimiter=',')\n",
    "        self.X_tr_simpli=self.X_tr[:,self.columns]\n",
    "        self.X_tr_simpli, __, self.y_tr_simpli, __ = train_test_split(self.X_tr_simpli, self.y_tr_simpli, test_size=(1-tr_size), random_state=42)\n",
    "        print('Simplificación finalizada')\n",
    "        print(np.shape(self.X_tr_simpli))\n",
    "\n",
    "    def use_simplify(self,boolean):\n",
    "        self.simplificador= boolean\n",
    "        \n",
    "    def grid_search_cv(self,classifier,parameters,metodo):\n",
    "        if (metodo == 0):\n",
    "            classif = OneVsRestClassifier(classifier)\n",
    "        elif (metodo==1):\n",
    "            classif = ClassifierChain(classifier)\n",
    "        model_tunning = GridSearchCV(classif, param_grid=parameters,cv=4,verbose=50, n_jobs=-1,scoring='accuracy')\n",
    "        if (self.simplificador):\n",
    "            model_tunning.fit(self.X_tr_simpli, self.y_tr_simpli)\n",
    "        else :\n",
    "            model_tunning.fit(self.X_tr, self.y_tr)\n",
    "        print (model_tunning.best_score_)\n",
    "        print (model_tunning.best_params_)\n",
    "        self.modelo = model_tunning.best_estimator_\n",
    "\n",
    "    def fitting_classifier(self):\n",
    "        if (self.simplificador):\n",
    "            self.clf=self.modelo.fit(self.X_tr,self.y_tr)\n",
    "        else :\n",
    "            self.clf=self.modelo\n",
    "\n",
    "    def pred_metr(self):\n",
    "        if (self.simplificador):\n",
    "            self.y_pred=self.clf.predict(self.X_tst)\n",
    "        else :\n",
    "            self.y_pred=self.clf.predict(self.X_tst)\n",
    "        self.metrics()\n",
    "    \n",
    "    def metrics(self):\n",
    "        accuracy=mtc.accuracy_score(self.y_tst,self.y_pred)\n",
    "        hamming=mtc.hamming_loss(self.y_tst,self.y_pred)\n",
    "        precision=mtc.precision_score(self.y_tst,self.y_pred,average='micro')\n",
    "        print(\"Total accuracy: \", accuracy)\n",
    "        print(\"Hamming loss: \", hamming)\n",
    "        print(\"Precision: \", precision)\n",
    "        print(mtc.classification_report(self.y_tst,self.y_pred))\n",
    "        #print(\"Accuracy per class:\")\n",
    "        aux=0\n",
    "        for i in range(np.shape(self.y_pred)[1]):\n",
    "            #print (\"Class \" ,i,\": \" ,mtc.accuracy_score(self.y_tst[:,i],y_pred[:,i]))\n",
    "            aux=aux+mtc.accuracy_score(self.y_tst[:,i],self.y_pred[:,i])\n",
    "        print(\"Accuracy media: \",aux/37)\n",
    "        distance_matrix=np.ones((37,37))*(1/37)\n",
    "        np.fill_diagonal(distance_matrix,0)\n",
    "        emd_aux=0\n",
    "        for i in range (np.shape(self.y_tst)[0]):\n",
    "            emd_aux= emd_aux + emd(self.y_tst[i,:].astype(float),self.y_pred[i,:].astype(float),distance_matrix)\n",
    "        \n",
    "        print(\"EMD: \", emd_aux/(np.shape(self.y_tst)[0]))\n",
    "        print(\"METRICAS DE LABEL RANKING: \")\n",
    "        spearman=scipy.stats.spearmanr(self.y_tst, self.y_pred)[0]\n",
    "        print(\"Spearmans Rank Correlation Coefficient: \", spearman)\n",
    "        kendall=scipy.stats.kendalltau(self.y_tst, self.y_pred)[0]\n",
    "        print(\"Kendall’s tau Correlation Coefficient: \",kendall)      \n",
    "        \n",
    "    def dnn(self,batch,epochs):\n",
    "        num_classes = 37\n",
    "        batch_size = batch\n",
    "        epochs = epochs\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience= 7)\n",
    "        self.model = Sequential()\n",
    "        input_shape=(np.shape(self.X_tr)[1],)\n",
    "        self.model.add(Dense(200, activation='relu',input_shape=input_shape))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(Dense(100, activation='relu'))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(Dense(100, activation='relu'))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(Dense(num_classes, activation='sigmoid'))\n",
    "        print(self.model.summary())\n",
    "        self.model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        hist = self.model.fit(self.X_tr, self.y_tr,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(self.X_tst, self.y_tst),callbacks=[es])\n",
    "        self.y_pred = self.model.predict(self.X_tst)\n",
    "        self.y_pred[self.y_pred>=0.5] = 1\n",
    "        self.y_pred[self.y_pred<0.5] = 0\n",
    "        self.metrics()\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduce el directorio del dataset../../Datasets/dataset/\n",
      "Dataset cargado y preprocesado\n",
      "Inicio del programa clasificador\n",
      "Vas a usar clasificación normal o redes neuronales? (0 o 1) :1\n",
      "Introduce el batch size: 128\n",
      "Introduce epochs: 20\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 200)               8901600   \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 37)                3737      \n",
      "=================================================================\n",
      "Total params: 8,935,537\n",
      "Trainable params: 8,935,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 93949 samples, validate on 1781 samples\n",
      "Epoch 1/20\n",
      "93949/93949 [==============================] - 130s 1ms/step - loss: 0.1381 - acc: 0.9572 - val_loss: 0.0907 - val_acc: 0.9661\n",
      "Epoch 2/20\n",
      "93949/93949 [==============================] - 125s 1ms/step - loss: 0.0812 - acc: 0.9708 - val_loss: 0.0838 - val_acc: 0.9700\n",
      "Epoch 3/20\n",
      "93949/93949 [==============================] - 127s 1ms/step - loss: 0.0682 - acc: 0.9748 - val_loss: 0.0837 - val_acc: 0.9699\n",
      "Epoch 4/20\n",
      "93949/93949 [==============================] - 124s 1ms/step - loss: 0.0595 - acc: 0.9776 - val_loss: 0.0857 - val_acc: 0.9699\n",
      "Epoch 5/20\n",
      "93949/93949 [==============================] - 127s 1ms/step - loss: 0.0531 - acc: 0.9799 - val_loss: 0.0901 - val_acc: 0.9700\n",
      "Epoch 6/20\n",
      "93949/93949 [==============================] - 125s 1ms/step - loss: 0.0485 - acc: 0.9815 - val_loss: 0.0953 - val_acc: 0.9696\n",
      "Epoch 7/20\n",
      "93949/93949 [==============================] - 128s 1ms/step - loss: 0.0447 - acc: 0.9830 - val_loss: 0.0985 - val_acc: 0.9701\n",
      "Epoch 8/20\n",
      "93949/93949 [==============================] - 125s 1ms/step - loss: 0.0415 - acc: 0.9842 - val_loss: 0.1027 - val_acc: 0.9697\n",
      "Epoch 9/20\n",
      "93949/93949 [==============================] - 124s 1ms/step - loss: 0.0391 - acc: 0.9851 - val_loss: 0.1084 - val_acc: 0.9704\n",
      "Epoch 10/20\n",
      "93949/93949 [==============================] - 123s 1ms/step - loss: 0.0370 - acc: 0.9859 - val_loss: 0.1105 - val_acc: 0.9700\n",
      "Epoch 11/20\n",
      "93949/93949 [==============================] - 121s 1ms/step - loss: 0.0353 - acc: 0.9866 - val_loss: 0.1135 - val_acc: 0.9700\n",
      "Epoch 12/20\n",
      "93949/93949 [==============================] - 122s 1ms/step - loss: 0.0338 - acc: 0.9872 - val_loss: 0.1188 - val_acc: 0.9696\n",
      "Epoch 13/20\n",
      "93949/93949 [==============================] - 122s 1ms/step - loss: 0.0326 - acc: 0.9877 - val_loss: 0.1216 - val_acc: 0.9693\n",
      "Epoch 14/20\n",
      "93949/93949 [==============================] - 125s 1ms/step - loss: 0.0314 - acc: 0.9881 - val_loss: 0.1246 - val_acc: 0.9696\n",
      "Epoch 15/20\n",
      "93949/93949 [==============================] - 126s 1ms/step - loss: 0.0305 - acc: 0.9885 - val_loss: 0.1258 - val_acc: 0.9691\n",
      "Epoch 16/20\n",
      "93949/93949 [==============================] - 126s 1ms/step - loss: 0.0298 - acc: 0.9888 - val_loss: 0.1303 - val_acc: 0.9693\n",
      "Epoch 17/20\n",
      "93949/93949 [==============================] - 126s 1ms/step - loss: 0.0290 - acc: 0.9891 - val_loss: 0.1336 - val_acc: 0.9696\n",
      "Epoch 18/20\n",
      "93949/93949 [==============================] - 124s 1ms/step - loss: 0.0283 - acc: 0.9893 - val_loss: 0.1335 - val_acc: 0.9692\n",
      "Epoch 19/20\n",
      "93949/93949 [==============================] - 124s 1ms/step - loss: 0.0276 - acc: 0.9897 - val_loss: 0.1364 - val_acc: 0.9695\n",
      "Epoch 20/20\n",
      "93949/93949 [==============================] - 124s 1ms/step - loss: 0.0270 - acc: 0.9899 - val_loss: 0.1403 - val_acc: 0.9695\n",
      "Total accuracy:  0.41212801796743403\n",
      "Hamming loss:  0.030532497685782357\n",
      "Precision:  0.7012195121951219\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.68      0.74       109\n",
      "           1       0.81      0.57      0.67       138\n",
      "           2       0.72      0.69      0.70       224\n",
      "           3       0.69      0.70      0.70       180\n",
      "           4       0.71      0.67      0.69       177\n",
      "           5       0.71      0.76      0.74       262\n",
      "           6       0.65      0.51      0.57        55\n",
      "           7       0.74      0.69      0.71       341\n",
      "           8       0.49      0.64      0.55        91\n",
      "           9       0.50      0.42      0.45        65\n",
      "          10       0.70      0.74      0.72       181\n",
      "          11       0.40      0.64      0.49        25\n",
      "          12       0.65      0.68      0.67       172\n",
      "          13       0.50      0.20      0.29        10\n",
      "          14       0.90      0.76      0.82       173\n",
      "          15       0.53      0.45      0.49       146\n",
      "          16       0.69      0.69      0.69       113\n",
      "          17       0.41      0.29      0.34        42\n",
      "          18       0.38      0.15      0.21        41\n",
      "          19       0.47      0.36      0.41        25\n",
      "          20       0.50      0.05      0.09        21\n",
      "          21       0.77      0.70      0.73       155\n",
      "          22       0.95      0.69      0.80        26\n",
      "          23       0.48      0.53      0.50        19\n",
      "          24       0.64      0.53      0.58        17\n",
      "          25       0.61      0.63      0.62        30\n",
      "          26       0.83      0.76      0.79        78\n",
      "          27       0.81      0.89      0.85        38\n",
      "          28       0.52      0.27      0.36        51\n",
      "          29       1.00      0.75      0.86        32\n",
      "          30       0.33      0.13      0.19        15\n",
      "          31       0.55      0.35      0.43        17\n",
      "          32       0.80      0.50      0.62        16\n",
      "          33       0.76      0.81      0.78        36\n",
      "          34       0.91      0.89      0.90        45\n",
      "          35       0.38      0.43      0.40        14\n",
      "          36       0.92      0.60      0.73        20\n",
      "\n",
      "   micro avg       0.70      0.65      0.67      3200\n",
      "   macro avg       0.66      0.56      0.59      3200\n",
      "weighted avg       0.70      0.65      0.67      3200\n",
      " samples avg       0.74      0.72      0.69      3200\n",
      "\n",
      "Accuracy media:  0.9694675023142175\n",
      "EMD:  0.02402233638860636\n",
      "METRICAS DE LABEL RANKING: \n",
      "Spearmans Rank Correlation Coefficient:  [[ 1.          0.24141361 -0.04033071 ... -0.01045581  0.050167\n",
      "  -0.02189403]\n",
      " [ 0.24141361  1.         -0.04659196 ... -0.00553787  0.01692108\n",
      "  -0.02485143]\n",
      " [-0.04033071 -0.04659196  1.         ... -0.06036791 -0.03611331\n",
      "   0.00725993]\n",
      " ...\n",
      " [-0.01045581 -0.00553787 -0.06036791 ...  1.          0.13818994\n",
      "  -0.01364762]\n",
      " [ 0.050167    0.01692108 -0.03611331 ...  0.13818994  1.\n",
      "  -0.00816429]\n",
      " [-0.02189403 -0.02485143  0.00725993 ... -0.01364762 -0.00816429\n",
      "   1.        ]]\n",
      "Kendall’s tau Correlation Coefficient:  0.657553483418536\n",
      "Quieres probar a entrenar con algo diferente o salir? :no\n",
      "Fin del programa clasificador\n"
     ]
    }
   ],
   "source": [
    "path_string=input('Introduce el directorio del dataset')\n",
    "clasificacion=TFM(path_string)\n",
    "print('Inicio del programa clasificador')\n",
    "\n",
    "while True:\n",
    "    ask6=int(input('Vas a usar clasificación normal o redes neuronales? (0 o 1) :'))\n",
    "    if (ask6==0):\n",
    "        ask2=int(input('Que clasificador vas a usar?: '))\n",
    "        clasificador=valores.clasificadores_dict[ask2]\n",
    "        metod=int(input('Que metodo vas a usar?: Introduce 0 para One vs Rest, o 1 para Classifier Chain'))\n",
    "\n",
    "        while True:\n",
    "            ask1=input(('Quieres usar simplificacion para entrenar el algoritmo?'))\n",
    "            if (ask1.lower()=='si'):\n",
    "                non_zero_data=input(('Introduce el numero de elementos no nulos por categoria que consideras aceptable'))\n",
    "                training_size=float(input('Introduce el porcentaje de tamano de datos de entrenamiento que quieres usar para simplificar'))\n",
    "                print('Realizando simplificacion del dataset...')\n",
    "                clasificacion.simplify_dataset(non_zero_data,training_size)\n",
    "                clasificacion.use_simplify(True)\n",
    "            elif (ask1.lower()=='no'):\n",
    "                clasificacion.use_simplify(False)\n",
    "            print(\"Iniciando tunning de los parámetros:\")\n",
    "            clasificacion.grid_search_cv(clasificador,valores.parametros_dict[(clasificador,metod)],metod)    \n",
    "            print(\"Tunning finalizado:\")  \n",
    "            ask5=input('Quieres continuar con el entrenamiento?')\n",
    "            if (ask5.lower()=='no'):\n",
    "                break\n",
    "\n",
    "            print(\"Entrenando el algoritmo con los mejores parametros...\")\n",
    "            clasificacion.fitting_classifier()\n",
    "            print(\"Entrenamiento finalizado, se procede a predecir las etiquetas\")\n",
    "            print(\"Mostrando metricas obtenidas\")\n",
    "            clasificacion.pred_metr()\n",
    "            ask3=input('Quieres probar a variar la simplificacion?')\n",
    "            if (ask3.lower()=='si'):\n",
    "                continue\n",
    "            elif (ask3.lower()=='no'):\n",
    "                break\n",
    "\n",
    "        ask4=input('Quieres probar con otro clasificador o finalizamos el programa?')\n",
    "        if (ask4.lower()=='si'):\n",
    "            continue\n",
    "        elif (ask4.lower()=='no'):\n",
    "            break\n",
    "    elif (ask6==1):\n",
    "        ask7 =int(input('Introduce el batch size: '))\n",
    "        ask8 =int(input('Introduce epochs: '))\n",
    "        clasificacion.dnn(ask7,ask8)\n",
    "        ask9 = input('Quieres probar a entrenar con algo diferente o salir? :')\n",
    "        if (ask9.lower()=='si'):\n",
    "            continue\n",
    "        elif (ask9.lower() =='no'):\n",
    "            break\n",
    "        \n",
    "print(\"Fin del programa clasificador\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "tfm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
