{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTS ##\n",
    "import scipy.sparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pyplot\n",
    "import sklearn.metrics as mtc\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import ClassifierChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = scipy.sparse.load_npz('../../Datasets/dataset/X_tr.npz')\n",
    "X_tst = scipy.sparse.load_npz('../../Datasets/dataset/X_tst.npz')\n",
    "y_tr = np.load('../../Datasets/dataset/y_tr.npy')\n",
    "y_tst = np.load('../../Datasets/dataset/y_tst.npy')\n",
    "# Se binarizan las caracteristicas \n",
    "y_tr=MultiLabelBinarizer().fit_transform(y_tr)\n",
    "y_tst=MultiLabelBinarizer().fit_transform(y_tst)\n",
    "# Escalado \n",
    "scaler = MaxAbsScaler().fit(X_tr)\n",
    "X_tr=scaler.transform(X_tr)\n",
    "X_tst=scaler.transform(X_tst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-f94b37ffe12d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# REDUCCIÓN DE DIMENSIÓN PRIMERA TÉCNICA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msvd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mX_tst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\truncated_svd.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[0mReduced\u001b[0m \u001b[0mversion\u001b[0m \u001b[0mof\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mThis\u001b[0m \u001b[0mwill\u001b[0m \u001b[0malways\u001b[0m \u001b[0mbe\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdense\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m         \"\"\"\n\u001b[1;32m--> 159\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    550\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# REDUCCIÓN DE DIMENSIÓN PRIMERA TÉCNICA (al final no se usó)\n",
    "svd = TruncatedSVD(n_components=17, n_iter=10, random_state=42)\n",
    "X_tr = svd.fit_transform(X_tr)  \n",
    "X_tst = svd.transform(X_tst)  \n",
    "print(svd.explained_variance_ratio_)\n",
    "print(svd.explained_variance_ratio_.sum())\n",
    "print(svd.singular_values_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REDUCCION TAMAÑO TRAINING Y ELIMINAR CARACTERISTICAS INSERVIBLES\n",
    "\n",
    "columns=np.array([])\n",
    "for i in range(np.shape(X_tr)[1]):\n",
    "    if X_tr[:,i].count_nonzero()> 250:\n",
    "        columns=np.append(columns,i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4295,)\n"
     ]
    }
   ],
   "source": [
    "#Se guardan las caracteristicas mas importantes para evitar repetir este proceso siempre\n",
    "np.savetxt('features250.txt',columns,delimiter=',')\n",
    "\n",
    "print(np.shape(columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se cargan las caracteristicas guardadas\n",
    "columns=np.loadtxt('features500.txt',delimiter=',')\n",
    "X_tr2=X_tr[:,columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reducir tamaño de muestra para un tunning mas rapido\n",
    "X_aux, __, y_aux, __ = train_test_split(X_tr2, y_tr, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3978115786224441\n",
      "{'estimator__C': 1, 'estimator__class_weight': None, 'estimator__solver': 'liblinear', 'estimator__tol': 0.01}\n"
     ]
    }
   ],
   "source": [
    "#PRIMERA SOLUCIÓN \n",
    "#tunning\n",
    "classif = OneVsRestClassifier(LogisticRegression())\n",
    "parameters = {\n",
    "    \"estimator__C\": [1],\n",
    "    \"estimator__solver\": [\"liblinear\"],\n",
    "    \"estimator__tol\": [0.01],\n",
    "    \"estimator__class_weight\":[None]\n",
    "}\n",
    "\n",
    "model_tunning = GridSearchCV(classif, param_grid=parameters,cv=4)\n",
    "\n",
    "model_tunning.fit(X_tr, y_tr)\n",
    "\n",
    "print (model_tunning.best_score_)\n",
    "print (model_tunning.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "          tol=0.01, verbose=0, warm_start=False),\n",
       "          n_jobs=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clasificador final\n",
    "classif=OneVsRestClassifier(LogisticRegression(C=1,tol=0.01,solver='liblinear'))\n",
    "classif.fit(X_tr,y_tr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total accuracy:  0.39247613700168443\n",
      "Hamming loss:  0.02899980272243046\n",
      "Precision:  0.7686536056690287\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.70      0.77       109\n",
      "           1       0.78      0.46      0.58       138\n",
      "           2       0.78      0.64      0.70       224\n",
      "           3       0.76      0.55      0.64       180\n",
      "           4       0.82      0.69      0.75       177\n",
      "           5       0.75      0.63      0.69       262\n",
      "           6       0.65      0.47      0.55        55\n",
      "           7       0.79      0.67      0.72       341\n",
      "           8       0.64      0.52      0.57        91\n",
      "           9       0.60      0.38      0.47        65\n",
      "          10       0.72      0.66      0.69       181\n",
      "          11       0.57      0.16      0.25        25\n",
      "          12       0.69      0.51      0.58       172\n",
      "          13       0.67      0.20      0.31        10\n",
      "          14       0.91      0.68      0.78       173\n",
      "          15       0.65      0.38      0.48       146\n",
      "          16       0.69      0.58      0.63       113\n",
      "          17       0.47      0.19      0.27        42\n",
      "          18       0.56      0.24      0.34        41\n",
      "          19       0.54      0.28      0.37        25\n",
      "          20       0.00      0.00      0.00        21\n",
      "          21       0.82      0.69      0.75       155\n",
      "          22       0.95      0.69      0.80        26\n",
      "          23       0.75      0.47      0.58        19\n",
      "          24       0.50      0.18      0.26        17\n",
      "          25       0.78      0.70      0.74        30\n",
      "          26       0.90      0.71      0.79        78\n",
      "          27       0.94      0.79      0.86        38\n",
      "          28       0.79      0.22      0.34        51\n",
      "          29       1.00      0.62      0.77        32\n",
      "          30       0.57      0.27      0.36        15\n",
      "          31       0.67      0.24      0.35        17\n",
      "          32       0.89      0.50      0.64        16\n",
      "          33       0.84      0.72      0.78        36\n",
      "          34       0.91      0.87      0.89        45\n",
      "          35       0.50      0.43      0.46        14\n",
      "          36       0.83      0.50      0.62        20\n",
      "\n",
      "   micro avg       0.77      0.58      0.66      3200\n",
      "   macro avg       0.72      0.49      0.57      3200\n",
      "weighted avg       0.76      0.58      0.65      3200\n",
      " samples avg       0.72      0.65      0.65      3200\n",
      "\n",
      "Accuracy per class:\n",
      "Class  0 :  0.9747332959011791\n",
      "Class  1 :  0.9483436271757439\n",
      "Class  2 :  0.9320606400898371\n",
      "Class  3 :  0.9371139809096013\n",
      "Class  4 :  0.9539584503088153\n",
      "Class  5 :  0.9152161706906232\n",
      "Class  6 :  0.9758562605277934\n",
      "Class  7 :  0.9028635597978664\n",
      "Class  8 :  0.9601347557551937\n",
      "Class  9 :  0.9679955081414935\n",
      "Class  10 :  0.9393599101628298\n",
      "Class  11 :  0.9865244244806288\n",
      "Class  12 :  0.9298147108366086\n",
      "Class  13 :  0.9949466591802358\n",
      "Class  14 :  0.9623806850084222\n",
      "Class  15 :  0.9326221224031443\n",
      "Class  16 :  0.9573273441886581\n",
      "Class  17 :  0.9758562605277934\n",
      "Class  18 :  0.9781021897810219\n",
      "Class  19 :  0.9865244244806288\n",
      "Class  20 :  0.9882088714205502\n",
      "Class  21 :  0.9601347557551937\n",
      "Class  22 :  0.9949466591802358\n",
      "Class  23 :  0.9927007299270073\n",
      "Class  24 :  0.9904548006737788\n",
      "Class  25 :  0.991577765300393\n",
      "Class  26 :  0.9837170129140932\n",
      "Class  27 :  0.9943851768669287\n",
      "Class  28 :  0.9758562605277934\n",
      "Class  29 :  0.9932622122403144\n",
      "Class  30 :  0.9921392476137002\n",
      "Class  31 :  0.991577765300393\n",
      "Class  32 :  0.9949466591802358\n",
      "Class  33 :  0.991577765300393\n",
      "Class  34 :  0.9943851768669287\n",
      "Class  35 :  0.9921392476137002\n",
      "Class  36 :  0.9932622122403144\n",
      "Accuracy media:  0.9710001972775696\n"
     ]
    }
   ],
   "source": [
    "#metricas\n",
    "y_pred=classif.predict(X_tst)\n",
    "accuracy=mtc.accuracy_score(y_tst,y_pred)\n",
    "hamming=mtc.hamming_loss(y_tst,y_pred)\n",
    "precision=mtc.precision_score(y_tst,y_pred,average='micro')\n",
    "print(\"Total accuracy: \", accuracy)\n",
    "print(\"Hamming loss: \", hamming)\n",
    "print(\"Precision: \", precision)\n",
    "print(mtc.classification_report(y_tst,y_pred))\n",
    "print(\"Accuracy per class:\")\n",
    "aux=0\n",
    "for i in range(np.shape(y_pred)[1]):\n",
    "    print (\"Class \" ,i,\": \" ,mtc.accuracy_score(y_tst[:,i],y_pred[:,i]))\n",
    "    aux=aux+mtc.accuracy_score(y_tst[:,i],y_pred[:,i])\n",
    "print(\"Accuracy media: \",aux/37)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEGUNDA SOLUCIÓN\n",
    "#tunning \n",
    "#Se cargan las caracteristicas guardadas\n",
    "columns=np.loadtxt('features250.txt',delimiter=',')\n",
    "X_tr2=X_tr[:,columns]\n",
    "#Reducir tamaño de muestra para un tunning mas rapido\n",
    "X_aux, __, y_aux, __ = train_test_split(X_tr2, y_tr, test_size=0.1, random_state=42)\n",
    "classif =  ClassifierChain(LogisticRegression())\n",
    "parameters = {\n",
    "    \"base_estimator__C\": [1,10,100,1000],\n",
    "    \"base_estimator__solver\": [\"liblinear\"],\n",
    "    \"base_estimator__tol\": [0.1,0.01,0.001,0.0001],\n",
    "    \"base_estimator__max_iter\":[10000]\n",
    "}\n",
    "\n",
    "model_tunning = GridSearchCV(classif, param_grid=parameters,cv=4,scoring='accuracy')\n",
    "#print(model_tunning.get_params().keys())\n",
    "\n",
    "model_tunning.fit(X_aux, y_aux)\n",
    "\n",
    "print (model_tunning.best_score_)\n",
    "print (model_tunning.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clasificador final\n",
    "chain = ClassifierChain(LogisticRegression(C=4,tol=0.01,solver='liblinear', max_iter= 100))\n",
    "chain.fit(X_tr, y_tr)\n",
    "y_pred2=chain.predict(X_tst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total accuracy:  0.4042672655811342\n",
      "Hamming loss:  0.02954610983808064\n",
      "Precision:  0.739946380697051\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.67      0.74       109\n",
      "           1       0.72      0.51      0.60       138\n",
      "           2       0.75      0.66      0.70       224\n",
      "           3       0.69      0.62      0.65       180\n",
      "           4       0.75      0.69      0.72       177\n",
      "           5       0.75      0.66      0.70       262\n",
      "           6       0.63      0.53      0.57        55\n",
      "           7       0.79      0.69      0.74       341\n",
      "           8       0.60      0.53      0.56        91\n",
      "           9       0.47      0.42      0.44        65\n",
      "          10       0.74      0.67      0.70       181\n",
      "          11       0.59      0.40      0.48        25\n",
      "          12       0.68      0.59      0.63       172\n",
      "          13       1.00      0.40      0.57        10\n",
      "          14       0.92      0.71      0.80       173\n",
      "          15       0.54      0.40      0.46       146\n",
      "          16       0.68      0.62      0.65       113\n",
      "          17       0.50      0.26      0.34        42\n",
      "          18       0.59      0.24      0.34        41\n",
      "          19       0.54      0.28      0.37        25\n",
      "          20       0.00      0.00      0.00        21\n",
      "          21       0.80      0.71      0.75       155\n",
      "          22       0.86      0.69      0.77        26\n",
      "          23       0.69      0.47      0.56        19\n",
      "          24       0.56      0.29      0.38        17\n",
      "          25       0.77      0.77      0.77        30\n",
      "          26       0.89      0.72      0.79        78\n",
      "          27       1.00      0.66      0.79        38\n",
      "          28       0.65      0.25      0.37        51\n",
      "          29       1.00      0.72      0.84        32\n",
      "          30       0.36      0.27      0.31        15\n",
      "          31       0.80      0.24      0.36        17\n",
      "          32       0.80      0.50      0.62        16\n",
      "          33       0.84      0.75      0.79        36\n",
      "          34       0.93      0.82      0.87        45\n",
      "          35       0.60      0.43      0.50        14\n",
      "          36       0.76      0.65      0.70        20\n",
      "\n",
      "   micro avg       0.74      0.60      0.66      3200\n",
      "   macro avg       0.70      0.53      0.59      3200\n",
      "weighted avg       0.73      0.60      0.66      3200\n",
      " samples avg       0.72      0.68      0.66      3200\n",
      "\n",
      "Accuracy per class:\n",
      "Class  0 :  0.9713644020213363\n",
      "Class  1 :  0.9472206625491297\n",
      "Class  2 :  0.9286917462099944\n",
      "Class  3 :  0.9331836047164515\n",
      "Class  4 :  0.9466591802358225\n",
      "Class  5 :  0.9174620999438517\n",
      "Class  6 :  0.9758562605277934\n",
      "Class  7 :  0.905670971364402\n",
      "Class  8 :  0.9578888265019652\n",
      "Class  9 :  0.9618192026951151\n",
      "Class  10 :  0.9427288040426727\n",
      "Class  11 :  0.9876473891072431\n",
      "Class  12 :  0.9337450870297586\n",
      "Class  13 :  0.9966311061201573\n",
      "Class  14 :  0.9663110612015722\n",
      "Class  15 :  0.9225154407636159\n",
      "Class  16 :  0.9573273441886581\n",
      "Class  17 :  0.9764177428411005\n",
      "Class  18 :  0.978663672094329\n",
      "Class  19 :  0.9865244244806288\n",
      "Class  20 :  0.9882088714205502\n",
      "Class  21 :  0.9595732734418866\n",
      "Class  22 :  0.9938236945536215\n",
      "Class  23 :  0.9921392476137002\n",
      "Class  24 :  0.9910162829870859\n",
      "Class  25 :  0.9921392476137002\n",
      "Class  26 :  0.9837170129140932\n",
      "Class  27 :  0.9927007299270073\n",
      "Class  28 :  0.9747332959011791\n",
      "Class  29 :  0.9949466591802358\n",
      "Class  30 :  0.9898933183604717\n",
      "Class  31 :  0.9921392476137002\n",
      "Class  32 :  0.9943851768669287\n",
      "Class  33 :  0.9921392476137002\n",
      "Class  34 :  0.9938236945536215\n",
      "Class  35 :  0.9932622122403144\n",
      "Class  36 :  0.9938236945536215\n",
      "Accuracy media:  0.9704538901619192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\94_ma\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\94_ma\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "accuracy=mtc.accuracy_score(y_tst,y_pred2)\n",
    "hamming=mtc.hamming_loss(y_tst,y_pred2)\n",
    "precision=mtc.precision_score(y_tst,y_pred2,average='micro')\n",
    "print(\"Total accuracy: \", accuracy)\n",
    "print(\"Hamming loss: \", hamming)\n",
    "print(\"Precision: \", precision)\n",
    "print(mtc.classification_report(y_tst,y_pred2))\n",
    "print(\"Accuracy per class:\")\n",
    "\n",
    "aux=0\n",
    "for i in range(np.shape(y_pred2)[1]):\n",
    "    print (\"Class \" ,i,\": \" ,mtc.accuracy_score(y_tst[:,i],y_pred2[:,i]))\n",
    "    aux=aux+mtc.accuracy_score(y_tst[:,i],y_pred2[:,i])\n",
    "print(\"Accuracy media: \",aux/37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEEP LEARNING\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(300,200,100,50), random_state=0)\n",
    "clf.fit(X_tr, y_tr)\n",
    "y_pred3=clf.predict(X_tst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total accuracy:  0.39191465468837733\n",
      "Hamming loss:  0.030031716163103024\n",
      "Precision:  0.7275437942601566\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.61      0.71       109\n",
      "           1       0.78      0.50      0.61       138\n",
      "           2       0.75      0.68      0.71       224\n",
      "           3       0.72      0.61      0.66       180\n",
      "           4       0.81      0.67      0.73       177\n",
      "           5       0.69      0.68      0.69       262\n",
      "           6       0.62      0.65      0.64        55\n",
      "           7       0.78      0.69      0.73       341\n",
      "           8       0.61      0.48      0.54        91\n",
      "           9       0.48      0.40      0.44        65\n",
      "          10       0.68      0.72      0.70       181\n",
      "          11       0.39      0.44      0.42        25\n",
      "          12       0.63      0.55      0.59       172\n",
      "          13       0.00      0.00      0.00        10\n",
      "          14       0.85      0.77      0.81       173\n",
      "          15       0.56      0.38      0.46       146\n",
      "          16       0.64      0.50      0.56       113\n",
      "          17       0.48      0.24      0.32        42\n",
      "          18       0.52      0.34      0.41        41\n",
      "          19       0.67      0.48      0.56        25\n",
      "          20       0.00      0.00      0.00        21\n",
      "          21       0.82      0.75      0.79       155\n",
      "          22       0.86      0.73      0.79        26\n",
      "          23       0.54      0.74      0.62        19\n",
      "          24       0.75      0.71      0.73        17\n",
      "          25       0.73      0.73      0.73        30\n",
      "          26       0.84      0.68      0.75        78\n",
      "          27       0.94      0.82      0.87        38\n",
      "          28       0.69      0.22      0.33        51\n",
      "          29       1.00      0.75      0.86        32\n",
      "          30       1.00      0.13      0.24        15\n",
      "          31       0.60      0.18      0.27        17\n",
      "          32       0.90      0.56      0.69        16\n",
      "          33       0.76      0.72      0.74        36\n",
      "          34       0.88      0.84      0.86        45\n",
      "          35       0.75      0.43      0.55        14\n",
      "          36       0.82      0.45      0.58        20\n",
      "\n",
      "   micro avg       0.73      0.61      0.66      3200\n",
      "   macro avg       0.69      0.54      0.59      3200\n",
      "weighted avg       0.72      0.61      0.65      3200\n",
      " samples avg       0.73      0.68      0.67      3200\n",
      "\n",
      "Accuracy per class:\n",
      "Class  0 :  0.9685569904548007\n",
      "Class  1 :  0.9505895564289725\n",
      "Class  2 :  0.9309376754632229\n",
      "Class  3 :  0.9371139809096013\n",
      "Class  4 :  0.9517125210555868\n",
      "Class  5 :  0.9084783829309376\n",
      "Class  6 :  0.9769792251544076\n",
      "Class  7 :  0.9028635597978664\n",
      "Class  8 :  0.9578888265019652\n",
      "Class  9 :  0.9623806850084222\n",
      "Class  10 :  0.9376754632229085\n",
      "Class  11 :  0.982594048287479\n",
      "Class  12 :  0.9253228523301515\n",
      "Class  13 :  0.9943851768669287\n",
      "Class  14 :  0.9651880965749579\n",
      "Class  15 :  0.9247613700168444\n",
      "Class  16 :  0.9505895564289725\n",
      "Class  17 :  0.9758562605277934\n",
      "Class  18 :  0.9775407074677148\n",
      "Class  19 :  0.9893318360471645\n",
      "Class  20 :  0.9882088714205502\n",
      "Class  21 :  0.9640651319483436\n",
      "Class  22 :  0.9943851768669287\n",
      "Class  23 :  0.9904548006737788\n",
      "Class  24 :  0.9949466591802358\n",
      "Class  25 :  0.9910162829870859\n",
      "Class  26 :  0.9803481190342505\n",
      "Class  27 :  0.9949466591802358\n",
      "Class  28 :  0.9747332959011791\n",
      "Class  29 :  0.9955081414935429\n",
      "Class  30 :  0.9927007299270073\n",
      "Class  31 :  0.9910162829870859\n",
      "Class  32 :  0.9955081414935429\n",
      "Class  33 :  0.9898933183604717\n",
      "Class  34 :  0.9932622122403144\n",
      "Class  35 :  0.9943851768669287\n",
      "Class  36 :  0.9927007299270073\n",
      "Accuracy media:  0.9699682838368968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\94_ma\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\94_ma\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#metricas\n",
    "accuracy=mtc.accuracy_score(y_tst,y_pred3)\n",
    "hamming=mtc.hamming_loss(y_tst,y_pred3)\n",
    "precision=mtc.precision_score(y_tst,y_pred3,average='micro')\n",
    "print(\"Total accuracy: \", accuracy)\n",
    "print(\"Hamming loss: \", hamming)\n",
    "print(\"Precision: \", precision)\n",
    "print(mtc.classification_report(y_tst,y_pred3))\n",
    "print(\"Accuracy per class:\")\n",
    "\n",
    "aux=0\n",
    "for i in range(np.shape(y_pred3)[1]):\n",
    "    print (\"Class \" ,i,\": \" ,mtc.accuracy_score(y_tst[:,i],y_pred3[:,i]))\n",
    "    aux=aux+mtc.accuracy_score(y_tst[:,i],y_pred3[:,i])\n",
    "print(\"Accuracy media: \",aux/37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
